<h1 align="center">
  ğŸš Real-Time Computer Vision & Deep Learning Drone Tracking System
</h1>

<p align="center">
  <b>AI-powered autonomous drone tracking with YOLOv8, gesture control, and manual override on DJI Tello</b>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Drone-DJI%20Tello-blue?style=for-the-badge&logo=dji" />
  <img src="https://img.shields.io/badge/Framework-YOLOv8-orange?style=for-the-badge&logo=opencv" />
  <img src="https://img.shields.io/badge/Deep%20Learning-PyTorch%20%7C%20TensorFlow-red?style=for-the-badge&logo=pytorch" />
  <img src="https://img.shields.io/badge/Computer%20Vision-OpenCV-green?style=for-the-badge&logo=opencv" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Status-Active-success?style=flat-square" />
  <img src="https://img.shields.io/github/license/Narein-karthik/Real-Time-Computer-Vision-and-Deep-Learning-Based-Drone-Tracking-System-using-DJI-Tello?style=flat-square" />
  <img src="https://img.shields.io/github/languages/top/Narein-karthik/Real-Time-Computer-Vision-and-Deep-Learning-Based-Drone-Tracking-System-using-DJI-Tello?style=flat-square" />
</p>

---

## ğŸ§  Project Description

This project presents a **real-time AI-powered drone tracking system** built using **computer vision and deep learning** on the DJI Tello platform. The system integrates YOLOv8-based detection for autonomous tracking, intelligent control algorithms, and humanâ€“drone interaction for advanced navigation capabilities.

It supports **autonomous tracking**, **gesture-based control**, and **manual override**, allowing seamless switching between AI-driven navigation and user control. GPU acceleration (CUDA) is utilized for high-performance inference, enabling low-latency real-time tracking.

---

## âœ¨ Key Features

- ğŸ¯ **YOLOv8-based real-time object/face tracking**  
- âœ‹ **Deep learningâ€“based hand gesture control**  
- âŒ¨ï¸ **Manual keyboard override for safety**  
- ğŸ¤– **AI-driven autonomous navigation**  
- âš¡ **GPU acceleration (CUDA support)**  
- ğŸ“¹ **Live video streaming and recording**  
- ğŸ” **Multi-mode control (AI + Gesture + Manual)**  
- ğŸ§© **Modular, extensible architecture**

---

## ğŸ—ï¸ System Architecture

The system is organized into layered modules:

### ğŸ›°ï¸ Perception Layer
- **YOLOv8** â€“ Object/face detection  
- **MediaPipe** â€“ Hand landmark detection  
- **OpenCV** â€“ Vision processing

### ğŸ§® Intelligence Layer
- **PyTorch** â€“ YOLO inference  
- **TensorFlow** â€“ Gesture recognition model

### ğŸ® Control & Interaction Layers
- **djitellopy** â€“ DJI Tello SDK  
- Gesture recognition and manual keyboard control

### âš™ï¸ Acceleration Layer
- **CUDA** for GPU-accelerated inference

---

## ğŸ›ï¸ Functional Modes

### ğŸ¤– Autonomous Tracking Mode
- YOLOv8-based detection  
- Target locking and distance regulation  
- PID-style control logic

### âœ‹ Hand Gesture Control Mode
- Hand landmark extraction  
- Deep learning classification  
- Gesture-to-command mapping

### ğŸ•¹ï¸ Manual Mode
- Keyboard override  
- Safety control and precise navigation

---

## ğŸ“ Project Structure

```bash
ai-drone-tracking-system/
â”‚
â”œâ”€â”€ tracking/
â”‚   â””â”€â”€ face_tracking.py          # YOLOv8-based face/object tracking
â”‚
â”œâ”€â”€ gesture_control/
â”‚   â””â”€â”€ gesture_tracking.py       # Hand tracking and gesture control
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ README.md                 # Model documentation
â”‚
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ“¦ Dependencies

| Library       | Purpose                                                 |
| ------------- | ------------------------------------------------------- |
| djitellopy    | Communication and control interface for DJI Tello drone |
| opencv-python | Real-time video processing and visualization            |
| ultralytics   | YOLOv8 deep learning framework                          |
| torch         | GPU acceleration and deep learning inference            |
| tensorflow    | Gesture recognition model framework (.h5)               |
| mediapipe     | Hand landmark detection for gesture extraction          |
| numpy         | Numerical computation and preprocessing                 |
| keyboard      | Manual keyboard-based drone control                     |

---

## ğŸ”§ Installation

```bash
git clone https://github.com/Narein-karthik/Real-Time-Computer-Vision-and-Deep-Learning-Based-Drone-Tracking-System-using-DJI-Tello.git
cd Real-Time-Computer-Vision-and-Deep-Learning-Based-Drone-Tracking-System-using-DJI-Tello
pip install -r requirements.txt
```

---

## ğŸš€ Execution

### ğŸ¯ Face/Object Tracking Mode

```bash
python tracking/face_tracking.py
```

**Features:**
- Automatically locks onto a detected face/object  
- Adjusts drone orientation and distance using control logic

### âœ‹ Gesture Control Mode

```bash
python gesture_control/gesture_tracking.py
```

**Example gesture mapping:**
- Open palm: Takeoff  
- Closed fist: Land  
- Swipe left/right: Move left/right  
- Palm forward: Move forward/backward

---

## ğŸŒ Applications

- ğŸ” **Autonomous surveillance systems**  
- ğŸ“¡ **Smart monitoring solutions**  
- ğŸ¤ **Humanâ€“robot interaction**  
- ğŸ§ª **AI robotics research and education**  
- ğŸ™ï¸ **Smart city robotics and automation**

---

## ğŸš§ Future Enhancements

- ğŸ‘ï¸â€ğŸ—¨ï¸ Multi-object tracking with priority queue  
- ğŸ Swarm drone coordination  
- ğŸ—£ï¸ Voice-based control integration  
- ğŸ§­ Autonomous path planning and obstacle avoidance  
- â˜ï¸ Cloud + Edge AI optimization  
- ğŸ“± Mobile appâ€“based control interface

---

## ğŸ“š Publications & Reports

### ğŸ“œ Published Research Paper

**Title:** AUTO TRACK: SMART AERIAL OBJECT TRACKING WITH DEEP LEARNING

**Authors:** Chethana G M, Dhanush M, Narein Karthik E, Nithin P  
**Advisor:** Dhivya R (Assistant Professor, AIML, Bangalore Technological Institute)

**Published in:** International Research Journal of Modernization in Engineering Technology and Science (IRJMETS)  
**Volume:** 07 | **Issue:** 09 | **Month:** September 2025  
**DOI:** [10.56726/IRJMETS83167](https://www.doi.org/10.56726/IRJMETS83167)  
**e-ISSN:** 2582-5208  
**Impact Factor:** 8.187  
**Publication Date:** October 1, 2025

**Journal Website:** [www.irjmets.com](http://www.irjmets.com)

### ğŸ¯ Paper Highlights

- **Object Detection & Tracking:** YOLOv8n-based model optimized for drone vision with multi-object tracking
- **Obstacle Avoidance:** Computer vision with depth estimation and path correction algorithms
- **Gesture Recognition:** Reinforcement Learning CNN for real-time 3D hand gesture control
- **Flight Control:** PID-based control loop integration for stability and maneuverability
- **Safety Module:** 3D-printed detachable ducted propellers (10g weight) for enhanced flight stability

### ğŸ“„ Access Full Report

> ğŸ“ **Download:** [Full Research Paper PDF](docs/IRJMETS70900072870-1st-publication-report.pdf)  
> ğŸ¯ **Certificate:** [Publication Certificate PDF](docs/IRJMETS70900072870-4-1st-publication-certificate.pdf)

**Note:** Upload the PDF files to a `/docs` folder in your repository for the links to work.

---

## ğŸ‘¨â€ğŸ’» Authors

- **Narein Karthik E** â€“ AI & ML Student (Computer Vision, Robotics)  
- **Nithin P** â€“ AI & ML Student (Deep Learning, Control Systems)  
- **Dhanush M** â€“ AI & ML Student (Integration & Testing)  
- **Chethana G M** â€“ AI & ML Student (Humanâ€“Computer Interaction)

---

## ğŸ™ Acknowledgements

- DJI Tello SDK (djitellopy) for drone communication  
- Ultralytics YOLOv8 for real-time detection  
- Google MediaPipe for robust hand landmark detection

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

<p align="center">Made with â¤ï¸ by the AI & ML Team at Bangalore Technological Institute</p>
